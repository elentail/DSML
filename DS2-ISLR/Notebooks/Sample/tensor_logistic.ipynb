{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "filter_df = df.drop(['Time','Amount'],axis=1)\n",
    "normal_df = filter_df[filter_df['Class'] == 0]\n",
    "fraud_df = filter_df[filter_df['Class'] == 1]\n",
    "\n",
    "dataset = pd.concat([normal_df.sample(3*len(fraud_df)),fraud_df])\n",
    "\n",
    "X_mat = dataset.drop('Class',axis=1).as_matrix()\n",
    "U,S,VT = np.linalg.svd(X_mat.T.dot(X_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = zip(dataset.drop('Class',axis=1).columns, np.abs(U[:,0]))\n",
    "pairs = sorted(pairs,key=lambda x:x[1],reverse=True)\n",
    "newx,newy = zip(*pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_var = sorted(list(newx[:14]),key=lambda x:int(x[1:]))\n",
    "#selected_var.append('Class')\n",
    "down_sample = dataset[selected_var]\n",
    "\n",
    "feature_x = scale(dataset[selected_var])\n",
    "feature_y = dataset['Class']\n",
    "\n",
    "design_matrix = np.column_stack([feature_x, feature_y.as_matrix()])\n",
    "train_data,test_data = train_test_split(design_matrix,test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def get_pipe(dataset,batch_size=10):\n",
    "    data = tf.data.Dataset.from_tensor_slices( tf.cast(dataset,dtype=tf.float32) ) \n",
    "    data = data.shuffle(3000).repeat().batch(batch_size)\n",
    "    batch = data.make_one_shot_iterator().get_next()\n",
    "    return batch[:,:-1], tf.cast(batch[:,-1],tf.int32)\n",
    "\n",
    "def get_model(input):\n",
    "    \n",
    "    with tf.variable_scope('Layer1'):\n",
    "        w1 = tf.get_variable('W1',shape=[14,20], initializer=tf.random_normal_initializer)\n",
    "        b1 = tf.get_variable('B1',shape=[20], initializer=tf.random_normal_initializer)\n",
    "        l1 = tf.nn.relu( tf.matmul(input,w1) + b1 )\n",
    "        \n",
    "    with tf.variable_scope('Layer2'):\n",
    "        w2 = tf.get_variable('W2',shape=[20,10], initializer=tf.random_normal_initializer)\n",
    "        b2 = tf.get_variable('B2',shape=[10], initializer=tf.random_normal_initializer)\n",
    "        l2 = tf.nn.relu( tf.matmul(l1,w2) + b2 )\n",
    "        \n",
    "    with tf.variable_scope('Layer3'):\n",
    "        w3 = tf.get_variable('W3',shape=[10,2], initializer=tf.random_normal_initializer)\n",
    "        b3 = tf.get_variable('B3',shape=[2], initializer=tf.random_normal_initializer)\n",
    "        y = tf.matmul(l2,w3) + b3\n",
    "        \n",
    "    return y\n",
    "\n",
    "\n",
    "def get_loss(logits,labels):\n",
    "\n",
    "    # Cost function: Cross Entropy\n",
    "    #cost = -tf.reduce_sum(label * tf.log(pred))\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits( labels=labels, logits=logits, name='cross_entropy')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy_mean')\n",
    "    \n",
    "    ################################################\n",
    "    ## Record the loss value (cross_entropy_mean) ##\n",
    "    ################################################\n",
    "    \n",
    "    tf.summary.scalar('cross_entropy_mean', cross_entropy_mean)\n",
    "    reg_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if v.name[0] != 'B']) * 0.001\n",
    "    \n",
    "    return cross_entropy_mean + reg_loss\n",
    "\n",
    "\n",
    "def train(loss,learning_rate=0.001):\n",
    "    \n",
    "    return tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    \n",
    "def accuracy(logits, labels):\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    score = tf.reduce_sum(tf.cast(tf.equal(predictions, labels), tf.int32))\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow multi layer regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 0.42700353\n",
      "199 0.38728642\n",
      "299 0.065513745\n",
      "399 0.005765436\n",
      "0.005436239\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def get_model(batch):\n",
    "    with tf.variable_scope('layer1'):\n",
    "        w1 = tf.get_variable('W1',shape=[1,20],initializer=tf.random_normal_initializer)\n",
    "        b1 = tf.get_variable('B1',shape=[20],initializer=tf.zeros_initializer)\n",
    "        l1 = tf.nn.sigmoid( tf.matmul(batch,w1)+ b1 )\n",
    "        \n",
    "    with tf.variable_scope('layer2'):\n",
    "        w2 = tf.get_variable('W2',shape=[20,20],initializer=tf.random_normal_initializer)\n",
    "        b2 = tf.get_variable('B2',shape=[20],initializer=tf.zeros_initializer)\n",
    "        l2 = tf.nn.sigmoid(tf.matmul(l1,w2)+ b2)\n",
    "        \n",
    "    with tf.variable_scope('layer3'):\n",
    "        w3 = tf.get_variable('W3',shape=[20,1],initializer=tf.random_normal_initializer)\n",
    "        b3 = tf.get_variable('B3',shape=[1],initializer=tf.zeros_initializer)\n",
    "        y = tf.matmul(l2,w3)+ b3\n",
    "        \n",
    "    return y\n",
    "        \n",
    "def get_loss(pred,label):\n",
    "    loss = tf.reduce_mean(tf.square(pred - label))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def parse_csv(line):\n",
    "    tokens = tf.string_split([line],delimiter=',')\n",
    "    return tokens.values\n",
    "\n",
    "# 같은 모델을 사용해도 성능이 좋지 않은데 이유를 모르겠음\n",
    "# interleave 를 통해 변환하는 과정에서 문제가 있는 것으로 추정\n",
    "def get_train(files):\n",
    "    train = tf.data.Dataset.from_tensor_slices(files)\n",
    "    train = train.interleave(lambda x:tf.data.TextLineDataset(x),cycle_length=2)\n",
    "    #train = train.shuffle(200).repeat()\n",
    "    train = train.repeat()\n",
    "    train = train.map(parse_csv).map(lambda x:tf.string_to_number(x)).batch(102)\n",
    "    return train\n",
    "\n",
    "# 같은 모델을 사용해도 성능이 좋지 않은데 이유를 모르겠음\n",
    "# interleave 를 통해 변환하는 과정에서 문제가 있는 것으로 추정\n",
    "def get_valid(files):\n",
    "    valid = tf.data.Dataset.from_tensor_slices(files)\n",
    "    valid = valid.interleave(lambda x:tf.data.TextLineDataset(x),cycle_length=2)\n",
    "    valid = valid.map(parse_csv).map(lambda x:tf.string_to_number(x)).batch(10)\n",
    "    return valid\n",
    "\n",
    "\n",
    "def get_text(file):\n",
    "    text = tf.data.TextLineDataset(file)\n",
    "    text = text.map(parse_csv).map(lambda x:tf.string_to_number(x)).repeat()\n",
    "    text = text.batch(30).prefetch(1)\n",
    "    #return text.make_one_shot_iterator().get_next()\n",
    "    return text\n",
    "    \n",
    "    \n",
    "def get_memory(file):\n",
    "    data = np.matrix(pd.read_csv(file).as_matrix() ,dtype=np.float32)\n",
    "    data = tf.data.Dataset.from_tensor_slices(data)\n",
    "    data = data.batch(102).repeat()\n",
    "    #return data.make_one_shot_iterator().get_next()\n",
    "    return data\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "#xor : cos(x)\n",
    "flist1 = ['/home/roadking/Downloads/xor.csv']\n",
    "flist2 = ['/home/roadking/Downloads/test.csv']\n",
    "\n",
    "\n",
    "#train_set = get_train(flist1)\n",
    "#valid_set = get_valid(flist2)\n",
    "\n",
    "\n",
    "train_set = get_memory(flist1[0])\n",
    "valid_set = get_memory(flist2[0])\n",
    "\n",
    "#train_set = get_text(flist1[0])\n",
    "#valid_set = get_text(flist2[0])\n",
    "\n",
    "\n",
    "iterator = tf.data.Iterator.from_structure(train_set.output_types,train_set.output_shapes)\n",
    "batch = iterator.get_next()\n",
    "\n",
    "train_iter = iterator.make_initializer(train_set)\n",
    "valid_iter = iterator.make_initializer(valid_set)\n",
    "\n",
    "\n",
    "pred = get_model(batch[:,:1])\n",
    "loss = get_loss(pred,batch[:,1:])\n",
    "train_op = tf.train.AdamOptimizer(0.05).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#with tf.train.MonitoredTrainingSession() as sess:\n",
    "with tf.Session() as sess:\n",
    "    sess.run([init,train_iter])\n",
    "    for i in range(400):\n",
    "        \n",
    "        _,error = sess.run([train_op,loss])\n",
    "        \n",
    "        if(i+1)%100 == 0:\n",
    "            print(i,error)   \n",
    "\n",
    "    sess.run(valid_iter)\n",
    "    print(sess.run(loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "df = pd.read_csv('/home/roadking/Downloads/xor.csv')\n",
    "dt = pd.read_csv('/home/roadking/Downloads/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = df.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPRegressor(hidden_layer_sizes=(20,20,20),random_state=1,max_iter=800)\n",
    "clf.fit(data_set[:,:1],data_set[:,1])\n",
    "#clf.loss_curve_\n",
    "error = np.mean(np.square(dt.iloc[:,1] - clf.predict(dt.iloc[:,:1])))\n",
    "print(error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
